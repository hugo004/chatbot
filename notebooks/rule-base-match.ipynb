{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11686797903579700592 DEPATURE 0 1 From From IN\n",
      "762183493788757442 DESTINATION 7 9 to JP JP NNP\n",
      "11686797903579700592 DEPATURE 10 11 from from IN\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "# extract entity use matcher, and return the desire pattern only\n",
    "matcher = Matcher(nlp.vocab)\n",
    "patterns = [\n",
    "  [\n",
    "    {'lower': 'from'},\n",
    "    # {'text': {'regex': '\\w+'}},\n",
    "    # {'tag': {'regex': '^NNP?$'}}\n",
    "  ]\n",
    "]\n",
    "matcher.add('DEPATURE', patterns)\n",
    "matcher.add('DESTINATION', [[\n",
    "  {'lower': 'to'},\n",
    "  {'text': {'regex': '\\w+'}}\n",
    "]])\n",
    "\n",
    "doc = nlp('From the Japan, Hong Kong  to JP, from 12/12')\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "  string_id = nlp.vocab.strings[match_id]\n",
    "  phase_span = doc[start:end]\n",
    "  span = doc[end-1]\n",
    "  print(match_id, string_id, start, end, phase_span.text, span.text, span.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no support REGEX\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('depature', [nlp('from \\w+')])\n",
    "doc = nlp('from hk to jp')\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "  string_id = nlp.vocab.strings[match_id]\n",
    "  phase_span = doc[start:end]\n",
    "  span = doc[end-1]\n",
    "  print(match_id, string_id, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(from, US, Japan, Hong Kong, to, japan, US, from, 12/12, return date, 12/12/2022)\n",
      "from FROM 0 4 ['from']\n",
      "US GPE 9 11 ['US']\n",
      "Japan GPE 13 18 ['Japan']\n",
      "Hong Kong GPE 20 29 ['Hong', 'Kong']\n",
      "to TO 30 32 ['to']\n",
      "japan GPE 33 38 ['japan']\n",
      "US GPE 42 44 ['US']\n",
      "from DPTL 50 54 ['from']\n",
      "12/12 CARDINAL 55 60 ['12/12']\n",
      "return date RTND 65 76 ['return', 'date']\n",
      "12/12/2022 DATE 80 90 ['12/12/2022']\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "# return custom NER and other (extend the NER model)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "ruler = nlp.add_pipe('entity_ruler')\n",
    "patterns = [\n",
    "  {\n",
    "    'label': 'FROM',\n",
    "    'pattern': [\n",
    "      {'lower': 'from'}\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    'label': 'TO',\n",
    "    'pattern': [\n",
    "      {'lower': 'to'}\n",
    "    ]  \n",
    "  },\n",
    "  {\n",
    "    'label': 'DPTL', # depature location\n",
    "    'pattern': [\n",
    "      {'lemma': {'in': ['depature', 'from']}},\n",
    "      {'lower': {'in': ['location', 'place']}, 'op': '?'}\n",
    "      # {'text': {'regex': '.+'}},\n",
    "      # {'TAG': {'IN': ['NNP', 'NN']}},\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    'label': 'DSTT', # destination\n",
    "    'pattern': [\n",
    "      {'lemma': {'in':['destination', 'to']}},\n",
    "      {'lower': {'in': ['location', 'place'], 'op': '?'}}\n",
    "      # {'text': {'regex': '\\w+'}},\n",
    "      # {'tag': {'regex': 'NNP?'}},\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    'label': 'DPTD', # depature date\n",
    "    'pattern': [\n",
    "      {'lemma': {'in': ['depature', 'leave']}},\n",
    "      {'lower': {'in': ['time', 'date', 'at', 'in']}}\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    'label': 'RTND', # return date\n",
    "    'pattern': [\n",
    "      {'lemma': {'in': ['return', 'back']}},\n",
    "      {'lower': {'in': ['time', 'date', 'on', 'at', 'in']}}\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp('from the US, Japan, Hong Kong to japan or US, and from 12/12 and return date is 12/12/2022')\n",
    "print(doc.ents)\n",
    "for ent in doc.ents:\n",
    "  print(ent.text, ent.label_, ent.start_char, ent.end_char, ent.text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depature time DPTD\n",
      "leave date DPTD\n",
      "leaves time DPTD\n",
      "leaving date DPTD\n",
      "Leave time DPTD\n",
      "return time RTND\n",
      "Returns on RTND\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "doc = nlp('depature time, leave date, leaves time, leaving date, Leave time return time, return Date, Returns on')\n",
    "for ent in doc.ents:\n",
    "  print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('./model-extend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(book a, book is)\n",
      "book a ACTION 8 14 ['book', 'a']\n",
      "book is ACTION 27 34 ['book', 'is']\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "# return custom NER and other (extend the NER model)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "ruler = nlp.add_pipe('entity_ruler')\n",
    "patterns = [\n",
    "  {\n",
    "    'label': 'ACTION',\n",
    "    'pattern': [\n",
    "      {'lower': {'in': ['book']}}\n",
    "    ]\n",
    "  },\n",
    "]\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp('we want book a flight, the book is good. The flight')\n",
    "print(doc.ents)\n",
    "for ent in doc.ents:\n",
    "  print(ent.text, ent.label_, ent.start_char, ent.end_char, ent.text.split(' '))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75d55ca4f019643c96f8b7be8becca98ad24ff587f290d4cd4ab4ac6d98b5398"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('nlp-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
